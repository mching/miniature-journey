---
title: "Text Analysis of Security Now"
author: Mike Ching
date: "2017-07-01"
slug: securitynow-text-analysis
categories:
  - R
tags:
  - textmining
draft: true
---



<p>I wanted to try my hand at some text mining using the tidytext package, and I was wondering what I could look at. I love the podcast Security Now! by Steve Gibson and Leo Laporte, and I’ve always wondered if there was a way to build a good index of their podcast.</p>
<p>I used the <strong>tidyverse</strong> packages and <strong>tidytext</strong> by David Robinson. He and Julia Silge wrote <a href="http://tidytextmining.com/">the book</a> from which most of these methods came from.</p>
<pre class="r"><code>library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(tidytext)
library(ggplot2)
library(tidyr)</code></pre>
<p>I downloaded the episode transcripts from GRC.com.</p>
<pre><code>DIRECTORY &lt;- &quot;/Volumes/WD_6GB_Chings/media/Books/securitynow/shows/&quot;

# change the i to go from the first show number you want to the last show number. As of 1 July 2017, the most recent show is #618.

for(i in 500:599) {
  shortname &lt;- paste0(&quot;sn-&quot;, i, &quot;.txt&quot;)
  showname &lt;- paste0(&quot;https://www.grc.com/sn/&quot;, shortname)
  print(shortname)
  download.file(showname, destfile = paste0(DIRECTORY, shortname), method = &quot;curl&quot;)
}</code></pre>
<p>I loaded an example file and put it into a <a href="https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html">tibble.</a></p>
<pre class="r"><code>DIRECTORY &lt;- &quot;/Volumes/WD_6GB_Chings/media/Books/securitynow/shows/&quot;
# list.files(DIRECTORY)
sn_500 &lt;- readLines(paste0(DIRECTORY, &quot;sn-500.txt&quot;))
sn_500_df &lt;- data_frame(linenumber = 1:length(sn_500), text = sn_500, episode = 500L)</code></pre>
<p>Then I loaded the stop words data (words like “the”, “of” that are commonly used but not very meaningful). I added on Leo and Steve’s names and the word yeah which seemed to be used a lot.</p>
<pre class="r"><code>data(&quot;stop_words&quot;)
custom_stop_words &lt;- bind_rows(data_frame(word = c(&quot;leo&quot;, &quot;steve&quot;, &quot;yeah&quot;), 
                                          lexicon = rep(&quot;custom&quot;, 3)), 
                               stop_words)</code></pre>
<p>Then I used the</p>
<pre class="r"><code>sn_500_df %&gt;% unnest_tokens(word, text) %&gt;%
  anti_join(custom_stop_words) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  filter(n&gt;20) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(x = word, y = n)) +
  geom_col()  +
  xlab(NULL) +
  coord_flip()</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<p><img src="/post/2017-07-01-sn-text-mining_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>sn_500_df %&gt;% unnest_tokens(word, text) %&gt;%
  anti_join(custom_stop_words) %&gt;% 
  inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%
  count(episode, index = linenumber %/% 10, sentiment) %&gt;%
  spread(sentiment, n, fill = 0) %&gt;%
  mutate(netsentiment = positive - negative) %&gt;%
  ggplot(aes(x = index, y = netsentiment)) +
  geom_col()</code></pre>
<pre><code>## Joining, by = &quot;word&quot;
## Joining, by = &quot;word&quot;</code></pre>
<p><img src="/post/2017-07-01-sn-text-mining_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>bing_word_counts &lt;- sn_500_df %&gt;% unnest_tokens(word, text) %&gt;%
  anti_join(stop_words) %&gt;% 
  anti_join(data_frame(word = c(&quot;leo&quot;, &quot;steve&quot;, &quot;yeah&quot;))) %&gt;% 
  arrange(linenumber) %&gt;% 
  inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  ungroup()</code></pre>
<pre><code>## Joining, by = &quot;word&quot;
## Joining, by = &quot;word&quot;
## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>bing_word_counts %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) %&gt;%
  ungroup() %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = &quot;free_y&quot;) +
  labs(y = &quot;Contribution to sentiment&quot;,
       x = NULL) +
  coord_flip()</code></pre>
<pre><code>## Selecting by n</code></pre>
<p><img src="/post/2017-07-01-sn-text-mining_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Next step is to pull many episodes and compare them.</p>
<pre class="r"><code>sn_500_df %&gt;% unnest_tokens(character, text, token = &quot;characters&quot;) %&gt;%
  count(character, sort = T) %&gt;% mutate(total_chars = sum(n)) %&gt;%
  mutate(prop_chars = n/total_chars) %&gt;%
  arrange(character) %&gt;%
  ggplot(aes(x = character, y = prop_chars)) +
  geom_col()</code></pre>
<p><img src="/post/2017-07-01-sn-text-mining_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
