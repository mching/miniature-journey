---
title: "Text Analysis of Security Now"
author: Mike Ching
date: "2017-07-01"
slug: securitynow-text-analysis
categories:
  - R
tags:
  - textmining
draft: true
---

I wanted to try my hand at some text mining using the tidytext package, and I was wondering what I could look at. I love the podcast Security Now! by Steve Gibson and Leo Laporte, and I've always wondered if there was a way to build a good index of their podcast.



```{r}
library(dplyr)
library(tidytext)
library(ggplot2)
library(tidyr)

DIRECTORY <- "/Volumes/WD_6GB_Chings/media/Books/securitynow/shows/"
list.files(DIRECTORY)
sn_500 <- readLines(paste0(DIRECTORY, "sn-500.txt"))
sn_500_df <- data_frame(linenumber = 1:length(sn_500), text = sn_500, episode = 500L)

data("stop_words")
custom_stop_words <- bind_rows(data_frame(word = c("leo", "steve", "yeah"), 
                                          lexicon = rep("custom", 3)), 
                               stop_words)

sn_500_df %>% unnest_tokens(word, text) %>%
  anti_join(custom_stop_words) %>% 
  count(word, sort = TRUE) %>% 
  filter(n>20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col()  +
  xlab(NULL) +
  coord_flip()


sn_500_df %>% unnest_tokens(word, text) %>%
  anti_join(custom_stop_words) %>% 
  inner_join(get_sentiments("bing")) %>%
  count(episode, index = linenumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(netsentiment = positive - negative) %>%
  ggplot(aes(x = index, y = netsentiment)) +
  geom_col()


```

```{r}
bing_word_counts <- sn_500_df %>% unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  anti_join(data_frame(word = c("leo", "steve", "yeah"))) %>% 
  arrange(linenumber) %>% 
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

Next step is to pull many episodes and compare them.
```{r}
sn_500_df %>% unnest_tokens(character, text, token = "characters") %>%
  count(character, sort = T) %>% mutate(total_chars = sum(n)) %>%
  mutate(prop_chars = n/total_chars) %>%
  arrange(character) %>%
  ggplot(aes(x = character, y = prop_chars)) +
  geom_col()
```

